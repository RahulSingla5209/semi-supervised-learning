{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190e8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e78d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x225b5603f00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.device(\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2399849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Prepare the data\n",
    "This example uses the\n",
    "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
    "provided by the\n",
    "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "The task is binary classification\n",
    "to predict whether a person is likely to be making over USD 50,000 a year.\n",
    "The dataset includes 48,842 instances with 14 input features: 5 numerical features and 9 categorical features.\n",
    "First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n",
    "DataFrame:\n",
    "\"\"\"\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "\"\"\"\n",
    "Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels.\n",
    "\"\"\"\n",
    "# n = len(train_data)\n",
    "# train_data = train_data.sample(int(n/12)).reset_index(drop=True).copy()\n",
    "\n",
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")\n",
    "\n",
    "# train_data = pd.concat([train_data, test_data], axis=0).reset_index(drop=True).copy()\n",
    "\n",
    "\"\"\"\n",
    "Now we store the training and test data in separate CSV files.\n",
    "\"\"\"\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "\n",
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"native_country\",\n",
    "]\n",
    "\n",
    "train_data.drop(columns = NUMERIC_FEATURE_NAMES, inplace=True)\n",
    "train_data.drop(columns = \"income_bracket\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5460e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token count 102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[100,   9,  22,  68,  88,  86,  17,  64],\n",
       "       [ 99,   9,  20,  71,  87,  86,  17,  64],\n",
       "       [ 97,  11,  18,  73,  88,  86,  17,  64],\n",
       "       [ 97,   1,  20,  73,  87,  84,  17,  64],\n",
       "       [ 97,   9,  20,  77,  92,  84,  16,  30]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "all_data = []\n",
    "for f in CATEGORICAL_FEATURE_NAMES:\n",
    "    all_data = all_data + train_data[f].apply(lambda x: f+x).values.tolist()\n",
    "    \n",
    "n = len(train_data)\n",
    "cat_data = le.fit_transform(all_data).reshape(len(CATEGORICAL_FEATURE_NAMES),n).T\n",
    "cat_count = len(np.unique(cat_data))\n",
    "print(\"Total token count\", cat_count)\n",
    "cat_data[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621cc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_pos = random.randint(len(CATEGORICAL_FEATURE_NAMES), size=(n))\n",
    "masked_data = train_data[CATEGORICAL_FEATURE_NAMES].copy()\n",
    "mask_label = []\n",
    "for i in range(n):\n",
    "    mask_label = mask_label + [cat_data[i, masks_pos[i]]]\n",
    "    masked_data.iloc[i, masks_pos[i]] = \"mask\"+str(masks_pos[i])\n",
    "\n",
    "le2=LabelEncoder()\n",
    "mask_label = le2.fit_transform(mask_label)    \n",
    "masked_data[\"fnlwgt\"]=train_data[\"fnlwgt\"]\n",
    "train_data = masked_data\n",
    "masked_data[\"label\"] = mask_label\n",
    "CSV_HEADER = train_data.columns\n",
    "masked_data.head(10)\n",
    "\n",
    "\"\"\"\n",
    "Now we store the training and test data in separate CSV files.\n",
    "\"\"\"\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abda2990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native_country</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>mask5</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>77516</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>mask2</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>83311</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>mask6</td>\n",
       "      <td>United-States</td>\n",
       "      <td>215646</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>mask2</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>234721</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>mask6</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>338409</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>mask7</td>\n",
       "      <td>284582</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mask0</td>\n",
       "      <td>9th</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>160187</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>mask6</td>\n",
       "      <td>United-States</td>\n",
       "      <td>209642</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>mask4</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>45781</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>159449</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           workclass   education          marital_status          occupation  \\\n",
       "0          State-gov   Bachelors           Never-married        Adm-clerical   \n",
       "1   Self-emp-not-inc   Bachelors                   mask2     Exec-managerial   \n",
       "2            Private     HS-grad                Divorced   Handlers-cleaners   \n",
       "3            Private        11th                   mask2   Handlers-cleaners   \n",
       "4            Private   Bachelors      Married-civ-spouse      Prof-specialty   \n",
       "5            Private     Masters      Married-civ-spouse     Exec-managerial   \n",
       "6              mask0         9th   Married-spouse-absent       Other-service   \n",
       "7   Self-emp-not-inc     HS-grad      Married-civ-spouse     Exec-managerial   \n",
       "8            Private     Masters           Never-married      Prof-specialty   \n",
       "9            Private   Bachelors      Married-civ-spouse               mask3   \n",
       "\n",
       "     relationship    race   gender  native_country  fnlwgt  label  \n",
       "0   Not-in-family   mask5     Male   United-States   77516     83  \n",
       "1         Husband   White     Male   United-States   83311     20  \n",
       "2   Not-in-family   White    mask6   United-States  215646     17  \n",
       "3         Husband   Black     Male   United-States  234721     20  \n",
       "4            Wife   Black    mask6            Cuba  338409     16  \n",
       "5            Wife   White   Female           mask7  284582     61  \n",
       "6   Not-in-family   Black   Female         Jamaica  160187     93  \n",
       "7         Husband   White    mask6   United-States  209642     17  \n",
       "8           mask4   White   Female   United-States   45781     85  \n",
       "9         Husband   White     Male   United-States  159449     68  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f294a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Define dataset metadata\n",
    "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "the data into input features, and encoding the input features with respect to their types.\n",
    "\"\"\"\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bacd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all the input features.\n",
    "FEATURE_NAMES = CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"label\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = masked_data[\"label\"].unique()\n",
    "\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f32a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "The hyperparameters includes model architecture and training configurations.\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 8  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "\"\"\"\n",
    "## Implement data reading pipeline\n",
    "We define an input function that reads and parses the file, then converts features\n",
    "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "for training or evaluation.\n",
    "\"\"\"\n",
    "\n",
    "# target_label_lookup = layers.StringLookup(\n",
    "#     vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    "# )\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    # target_index = target_label_lookup(target)\n",
    "    target = int(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        #column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=True,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Implement a training and evaluation procedure\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694b97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create model inputs\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        # Get the vocabulary of the categorical feature.\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "        # Create a lookup to convert string values to an integer indices.\n",
    "        # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "        # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "        lookup = layers.StringLookup(\n",
    "            vocabulary=vocabulary,\n",
    "            mask_token=None,\n",
    "            num_oov_indices=1,\n",
    "            output_mode=\"int\",\n",
    "        )\n",
    "\n",
    "        # Convert the string input values into integer indices.\n",
    "        encoded_feature = lookup(inputs[feature_name])-1\n",
    "\n",
    "        # Create an embedding layer with the specified dimensions.\n",
    "        embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims,\n",
    "                name=feature_name+\"_embedding\"\n",
    "            )\n",
    "\n",
    "        # Convert the index values to embedding representations.\n",
    "        encoded_categorical_feature = embedding(encoded_feature)\n",
    "        encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement an MLP block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70445517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, filename):\n",
    "    embeddings = {}\n",
    "\n",
    "    for layer in model.layers: \n",
    "        if \"_embedding\" in  layer.get_config()[\"name\"]:\n",
    "            col_name = layer.get_config()[\"name\"].split(\"_embedding\")[0]\n",
    "            if col_name not in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "                continue\n",
    "            embeddings[col_name] = {}\n",
    "            for idx, cat in enumerate(CATEGORICAL_FEATURES_WITH_VOCABULARY[col_name]):\n",
    "                if \"mask\" in cat:\n",
    "                    continue\n",
    "                embeddings[col_name][cat] = layer.get_weights()[0][idx]\n",
    "            \n",
    "    with open(filename, 'wb') as config_dictionary_file:\n",
    "        pickle.dump(embeddings, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e7d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'workclass_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'education_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'marital_status_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'occupation_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'relationship_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'race_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'gender_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'native_country_embedding')>]\n",
      "Total model weights: 27826\n",
      "Tensor(\"StringToNumber:0\", shape=(None,), dtype=int32)\n",
      "Start training the model...\n",
      "Epoch 1/10\n",
      "123/123 [==============================] - 7s 25ms/step - loss: 347666.0312 - accuracy: 0.5528\n",
      "Epoch 2/10\n",
      "123/123 [==============================] - 2s 20ms/step - loss: 213740.1875 - accuracy: 0.6541\n",
      "Epoch 3/10\n",
      "123/123 [==============================] - 2s 19ms/step - loss: 195508.7500 - accuracy: 0.6748\n",
      "Epoch 4/10\n",
      "123/123 [==============================] - 2s 19ms/step - loss: 187717.0781 - accuracy: 0.6849\n",
      "Epoch 5/10\n",
      "123/123 [==============================] - 2s 20ms/step - loss: 182307.0469 - accuracy: 0.6889\n",
      "Epoch 6/10\n",
      "123/123 [==============================] - 2s 20ms/step - loss: 178791.9219 - accuracy: 0.6936\n",
      "Epoch 7/10\n",
      "123/123 [==============================] - 2s 19ms/step - loss: 175915.8750 - accuracy: 0.6961\n",
      "Epoch 8/10\n",
      "112/123 [==========================>...] - ETA: 0s - loss: 175527.0938 - accuracy: 0.6967"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    print(encoded_categorical_feature_list)\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten(name=\"dyanmic_embedding\")(encoded_categorical_features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * categorical_features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(categorical_features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=len(TARGET_LABELS), activation=\"softmax\", name=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "save_embeddings(tabtransformer_model, 'untrained_embeddings.dictionary')\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the TabTransformer model:\n",
    "\"\"\"\n",
    "\n",
    "history, tabtransformer_model = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(tabtransformer_model, 'unsupervised_trained_embeddings.dictionary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
