{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190e8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e78d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x27defb0db40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.device(\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bacd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Prepare the data\n",
    "This example uses the\n",
    "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
    "provided by the\n",
    "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "The task is binary classification\n",
    "to predict whether a person is likely to be making over USD 50,000 a year.\n",
    "The dataset includes 48,842 instances with 14 input features: 5 numerical features and 9 categorical features.\n",
    "First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n",
    "DataFrame:\n",
    "\"\"\"\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")\n",
    "\n",
    "\"\"\"\n",
    "Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels.\n",
    "\"\"\"\n",
    "\n",
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Now we store the training and test data in separate CSV files.\n",
    "\"\"\"\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)\n",
    "\n",
    "\"\"\"\n",
    "## Define dataset metadata\n",
    "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "the data into input features, and encoding the input features with respect to their types.\n",
    "\"\"\"\n",
    "\n",
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d752c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for x in list(CATEGORICAL_FEATURES_WITH_VOCABULARY.values()):\n",
    "    l = l + x\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df423236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NUMERIC_FEATURE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f32a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singl\\Anaconda3\\envs\\tabtrabsfomer\\lib\\site-packages\\numpy\\core\\numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "The hyperparameters includes model architecture and training configurations.\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "\"\"\"\n",
    "## Implement data reading pipeline\n",
    "We define an input function that reads and parses the file, then converts features\n",
    "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "for training or evaluation.\n",
    "\"\"\"\n",
    "\n",
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f91c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Implement a training and evaluation procedure\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694b97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create model inputs\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "                name = feature_name+\"_string_lookup\"\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims,\n",
    "                name=feature_name+\"_embedding\"\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement an MLP block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe595d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 109629\n",
      "Start training the model...\n",
      "Epoch 1/5\n",
      "    123/Unknown - 3s 8ms/step - loss: 109457.3203 - accuracy: 0.7509WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 18ms/step - loss: 109457.3203 - accuracy: 0.7509 - val_loss: 96728.1172 - val_accuracy: 0.7812\n",
      "Epoch 2/5\n",
      "121/123 [============================>.] - ETA: 0s - loss: 91232.6406 - accuracy: 0.7686WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 90963.2031 - accuracy: 0.7690 - val_loss: 70680.0078 - val_accuracy: 0.7957\n",
      "Epoch 3/5\n",
      "121/123 [============================>.] - ETA: 0s - loss: 75535.4844 - accuracy: 0.7951WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 9ms/step - loss: 75395.1719 - accuracy: 0.7956 - val_loss: 69629.5547 - val_accuracy: 0.8054\n",
      "Epoch 4/5\n",
      "123/123 [==============================] - ETA: 0s - loss: 72619.7422 - accuracy: 0.8014WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 8ms/step - loss: 72619.7422 - accuracy: 0.8014 - val_loss: 76765.0859 - val_accuracy: 0.7680\n",
      "Epoch 5/5\n",
      "119/123 [============================>.] - ETA: 0s - loss: 70784.4375 - accuracy: 0.8052WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 1s 9ms/step - loss: 70710.2734 - accuracy: 0.8052 - val_loss: 69048.6094 - val_accuracy: 0.8027\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "Validation accuracy: 80.27%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Experiment 1: a baseline model\n",
    "In the first experiment, we create a simple multi-layer feed-forward network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the baseline model:\n",
    "\"\"\"\n",
    "\n",
    "history = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6e7d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 87479\n",
      "Start training the model...\n",
      "Epoch 1/5\n",
      "    121/Unknown - 6s 22ms/step - loss: 80667.7734 - accuracy: 0.7982WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 8s 32ms/step - loss: 80299.5859 - accuracy: 0.7986 - val_loss: 68278.5547 - val_accuracy: 0.8299\n",
      "Epoch 2/5\n",
      "121/123 [============================>.] - ETA: 0s - loss: 69087.1094 - accuracy: 0.8252WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 26ms/step - loss: 68890.8516 - accuracy: 0.8253 - val_loss: 62984.8203 - val_accuracy: 0.8426\n",
      "Epoch 3/5\n",
      "122/123 [============================>.] - ETA: 0s - loss: 66111.2578 - accuracy: 0.8331WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 27ms/step - loss: 66099.8125 - accuracy: 0.8331 - val_loss: 63402.4805 - val_accuracy: 0.8434\n",
      "Epoch 4/5\n",
      "121/123 [============================>.] - ETA: 0s - loss: 64786.0352 - accuracy: 0.8367WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 64597.5430 - accuracy: 0.8370 - val_loss: 62349.4180 - val_accuracy: 0.8446\n",
      "Epoch 5/5\n",
      "121/123 [============================>.] - ETA: 0s - loss: 63912.3477 - accuracy: 0.8376WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 26ms/step - loss: 63765.6914 - accuracy: 0.8378 - val_loss: 62466.2578 - val_accuracy: 0.8457\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "Validation accuracy: 84.57%\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten(name=\"dyanmic_embedding\")(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the TabTransformer model:\n",
    "\"\"\"\n",
    "\n",
    "history, model = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac801df",
   "metadata": {},
   "source": [
    "# model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8057b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 2s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.4629639 , -0.25487277,  0.37423292, -0.27115652, -0.3749587 ,\n",
       "       -1.4549018 , -1.0544894 , -0.27981803,  0.4472498 ,  0.39431262,\n",
       "        0.9709523 , -0.6613591 ,  0.47682157,  0.67264247,  0.733248  ,\n",
       "       -1.1791941 ,  0.61067027, -1.4950783 ,  1.1776371 ,  0.7106426 ,\n",
       "       -0.02291129, -1.3071847 , -0.60640454,  1.1419406 ,  0.22963604,\n",
       "       -0.14083531, -0.47493035,  1.1826692 , -0.14184308,  0.04142279,\n",
       "       -0.47827542, -0.508345  ,  1.700352  ,  0.38867554, -0.32174838,\n",
       "       -0.9823497 , -0.34284875,  0.5556123 , -0.78804386, -0.10211467,\n",
       "        0.09900455,  1.402194  , -0.7659436 , -1.1014427 ,  0.31586397,\n",
       "        0.6919584 , -0.8240365 ,  0.3452404 ,  0.2460321 ,  0.5806065 ,\n",
       "        0.9471004 ,  1.6370349 ,  0.05321562,  0.66135687, -1.0480146 ,\n",
       "        0.59272283,  0.44058496, -0.5721347 , -1.1556574 , -0.4455008 ,\n",
       "       -1.1005421 , -0.0461639 , -0.8999513 ,  0.16597322,  0.96646297,\n",
       "       -0.7361055 ,  0.4314885 , -0.5602265 , -1.1830217 ,  0.3007879 ,\n",
       "       -1.628747  ,  0.8979627 , -0.31597346,  1.168677  ,  0.21746925,\n",
       "       -0.64920443,  0.413139  , -0.30669782,  0.8938744 ,  0.17581849,\n",
       "        1.1412729 , -1.631068  ,  0.8531904 , -0.3771115 , -0.24634826,\n",
       "       -1.6093614 , -0.4137379 ,  1.0918933 ,  0.53419626, -0.04575619,\n",
       "       -0.09936436,  1.0091459 ,  0.22728203, -0.13147953, -0.03771102,\n",
       "       -0.35031387,  1.3948338 ,  0.23446296,  0.84047776, -0.02297953,\n",
       "        1.1466701 , -0.61226255,  0.12653035, -0.43540975, -0.45482975,\n",
       "       -1.805886  ,  0.7726145 ,  0.11347555, -0.79594725, -0.6222294 ,\n",
       "       -0.6519139 ,  0.66397315,  0.75582707, -1.380872  ,  0.97498757,\n",
       "        1.1029083 , -0.5591279 , -0.07235028, -1.4206893 ,  0.8208377 ,\n",
       "       -0.46486852,  0.33230036,  0.62390804, -0.18205298, -0.00249139,\n",
       "       -1.0653069 ,  0.43815005,  0.10357544], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "train_dataset = get_dataset_from_csv(train_data_file, 265, shuffle=True)\n",
    "\n",
    "layer_name = 'dyanmic_embedding'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(train_dataset)\n",
    "intermediate_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15a2baaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workclass': {' ?': array([-0.002913  , -0.00553249, -0.03083848,  0.01469298,  0.04288837,\n",
       "          0.05423308, -0.01699628,  0.03633083, -0.00966665, -0.02456677,\n",
       "         -0.07572048,  0.01191209,  0.01783697,  0.00954238, -0.03047561,\n",
       "          0.01781413], dtype=float32),\n",
       "  ' Federal-gov': array([ 0.01310428, -0.03977202,  0.00368451,  0.03824069,  0.07260294,\n",
       "         -0.04431027, -0.00840496, -0.06366843,  0.03871363, -0.00249525,\n",
       "         -0.02176102, -0.01152009,  0.01050705,  0.05243825,  0.00859012,\n",
       "          0.00352215], dtype=float32),\n",
       "  ' Local-gov': array([ 0.01361209, -0.02620708, -0.00263902, -0.03982851,  0.00580543,\n",
       "          0.00190456, -0.0162914 ,  0.02812267,  0.05203782,  0.02736351,\n",
       "          0.05625531, -0.04260373,  0.03190786,  0.01787091, -0.030851  ,\n",
       "         -0.04142122], dtype=float32),\n",
       "  ' Never-worked': array([-0.03769368,  0.03724404, -0.04574641, -0.06064381, -0.00188336,\n",
       "         -0.00881192,  0.00527353, -0.05949204, -0.095719  ,  0.04534803,\n",
       "         -0.02995505,  0.02071888,  0.03510258,  0.01310332,  0.03851886,\n",
       "         -0.08364081], dtype=float32),\n",
       "  ' Private': array([-6.0376874e-03,  3.5307474e-02, -2.2519749e-02,  2.3701634e-05,\n",
       "          7.7957194e-03, -8.8573694e-03, -5.6705149e-03, -1.2105708e-02,\n",
       "         -1.4995156e-02,  1.8747721e-02,  5.5884201e-02, -2.2444258e-02,\n",
       "         -8.2406728e-03,  3.8442016e-02,  4.5225207e-02, -4.8707671e-02],\n",
       "        dtype=float32),\n",
       "  ' Self-emp-inc': array([-0.03406444,  0.0477611 , -0.01257205,  0.0160357 ,  0.04917194,\n",
       "         -0.03315839, -0.04199927,  0.00621177,  0.03056242,  0.06338087,\n",
       "         -0.00790445,  0.00402572, -0.02308439, -0.0160845 , -0.03942534,\n",
       "         -0.02954807], dtype=float32),\n",
       "  ' Self-emp-not-inc': array([ 0.03405108,  0.0248959 , -0.02273488,  0.05570329, -0.07303125,\n",
       "         -0.00083178, -0.00841205,  0.01210865, -0.0103841 , -0.00541879,\n",
       "         -0.01386601,  0.06545915,  0.04960861, -0.01332924,  0.00830499,\n",
       "          0.0398194 ], dtype=float32),\n",
       "  ' State-gov': array([-0.06222041, -0.01509997, -0.00528651,  0.05160764,  0.00722006,\n",
       "          0.0003277 ,  0.0094655 ,  0.03747341,  0.02738098, -0.06722627,\n",
       "         -0.0603465 , -0.04904568, -0.03857422, -0.00554896,  0.02228707,\n",
       "         -0.03823579], dtype=float32),\n",
       "  ' Without-pay': array([-0.01341056, -0.01231752, -0.02458694,  0.01155275, -0.01772678,\n",
       "          0.07788368, -0.032826  ,  0.03037973,  0.00674609, -0.03029977,\n",
       "          0.00485613, -0.03243394,  0.01707885,  0.0261231 ,  0.02659451,\n",
       "         -0.02280862], dtype=float32)},\n",
       " 'education': {' 10th': array([-0.06057727, -0.01321356, -0.00016063,  0.00865537,  0.0310176 ,\n",
       "         -0.02528814, -0.08233099,  0.02148969,  0.02334316,  0.04079723,\n",
       "         -0.01157407,  0.00036832,  0.03000831,  0.07145185, -0.00066511,\n",
       "         -0.0184255 ], dtype=float32),\n",
       "  ' 11th': array([-0.02199655,  0.05454277,  0.03244402,  0.03192561, -0.04510613,\n",
       "          0.01720498, -0.07452183,  0.00880622,  0.02683623,  0.0560654 ,\n",
       "         -0.00437363, -0.03018951, -0.01998599,  0.06443363, -0.0241464 ,\n",
       "         -0.03038274], dtype=float32),\n",
       "  ' 12th': array([ 0.00014194, -0.02469768, -0.00362065, -0.00527466, -0.02589248,\n",
       "         -0.03920004,  0.00060918,  0.04694302,  0.01125697,  0.03172564,\n",
       "         -0.05076497,  0.04962972,  0.02823359,  0.00616655,  0.02257074,\n",
       "         -0.0480989 ], dtype=float32),\n",
       "  ' 1st-4th': array([-0.03330378,  0.03105308, -0.03969208,  0.00316037, -0.02303049,\n",
       "         -0.01341888,  0.00899742, -0.05602111, -0.0032198 ,  0.05218802,\n",
       "         -0.08067796, -0.01746798,  0.00670773,  0.06296269, -0.00144188,\n",
       "          0.00404501], dtype=float32),\n",
       "  ' 5th-6th': array([-0.07295813,  0.03564278, -0.01948167, -0.00867424,  0.01332759,\n",
       "          0.00107992, -0.03011029, -0.01447401,  0.04042824,  0.02756049,\n",
       "         -0.03484493,  0.01038874,  0.00681493,  0.07806599, -0.03456598,\n",
       "         -0.02094361], dtype=float32),\n",
       "  ' 7th-8th': array([-0.01636247,  0.03060942, -0.04226039, -0.00838441,  0.00984848,\n",
       "         -0.00597345, -0.02822411, -0.06040432,  0.03150614,  0.08018004,\n",
       "         -0.0721878 , -0.02482893,  0.03437129,  0.01921801,  0.05012791,\n",
       "         -0.04080329], dtype=float32),\n",
       "  ' 9th': array([-0.06833444,  0.05358123, -0.04698168, -0.05716603, -0.02406461,\n",
       "          0.00430059,  0.00343904, -0.04050761,  0.02208525,  0.03508562,\n",
       "         -0.03095543, -0.02777171, -0.05571537,  0.10143974,  0.0208466 ,\n",
       "         -0.03097704], dtype=float32),\n",
       "  ' Assoc-acdm': array([ 0.01527652, -0.01218251,  0.01037597,  0.05249286,  0.04364464,\n",
       "         -0.00039707, -0.0162501 , -0.00491395,  0.02914092,  0.00386645,\n",
       "         -0.00354586,  0.01458668,  0.02237869, -0.07232549,  0.02850112,\n",
       "         -0.04497885], dtype=float32),\n",
       "  ' Assoc-voc': array([ 0.05689546, -0.01002144,  0.04949069,  0.03949187,  0.02912991,\n",
       "         -0.01844754,  0.03591299, -0.03262524,  0.00634851,  0.00860489,\n",
       "         -0.00539797, -0.01881224, -0.03379947, -0.01978588,  0.04482503,\n",
       "         -0.01242254], dtype=float32),\n",
       "  ' Bachelors': array([ 0.04387671, -0.00987456,  0.00811335, -0.03634793, -0.03561703,\n",
       "         -0.00170734, -0.00897382,  0.04347044,  0.02154377, -0.04908514,\n",
       "          0.01909728, -0.04400369, -0.03944825, -0.03231555, -0.00809499,\n",
       "         -0.00098256], dtype=float32),\n",
       "  ' Doctorate': array([ 0.01609265,  0.0615317 , -0.01457935, -0.03894489, -0.04595539,\n",
       "          0.02655026,  0.03361621, -0.05654623,  0.0109929 , -0.07249575,\n",
       "          0.04239861, -0.03756014, -0.01654569, -0.084953  ,  0.01960417,\n",
       "          0.03371172], dtype=float32),\n",
       "  ' HS-grad': array([-0.0175637 , -0.06582893,  0.01875849, -0.01128224, -0.05097735,\n",
       "         -0.0616764 , -0.02071728,  0.03734045, -0.00203984,  0.0004726 ,\n",
       "         -0.03950248,  0.02405401, -0.00567895, -0.00593232, -0.0134415 ,\n",
       "         -0.03158823], dtype=float32),\n",
       "  ' Masters': array([ 0.06716554, -0.0275126 ,  0.00515678, -0.01281285,  0.02587234,\n",
       "          0.04706972,  0.03408021,  0.00068484, -0.00232793, -0.06538805,\n",
       "          0.05683971, -0.03809338, -0.03241429, -0.01082986, -0.01223526,\n",
       "          0.01237749], dtype=float32),\n",
       "  ' Preschool': array([-0.05664983,  0.04019254, -0.0525113 , -0.04358651,  0.01643927,\n",
       "         -0.00387835, -0.00714741, -0.00728861,  0.01785914,  0.0673723 ,\n",
       "         -0.06796181, -0.00527451, -0.00096584,  0.03297366,  0.02871785,\n",
       "         -0.0451357 ], dtype=float32),\n",
       "  ' Prof-school': array([-0.00674278,  0.04068746, -0.05254553, -0.03736155, -0.02621817,\n",
       "          0.09934054,  0.0013632 ,  0.04023669, -0.02740614, -0.04069481,\n",
       "          0.03845766, -0.03193238,  0.00525464, -0.01032451, -0.02817247,\n",
       "          0.0610288 ], dtype=float32),\n",
       "  ' Some-college': array([-8.6850617e-03, -4.8439432e-02, -4.4083957e-02, -1.9778272e-02,\n",
       "         -3.7836682e-02, -5.2410074e-02, -2.3906616e-02, -1.7802503e-02,\n",
       "         -1.9339385e-02, -3.6388718e-02, -8.6981263e-03,  5.0850362e-03,\n",
       "         -2.1644967e-02,  6.5259362e-04, -2.5780988e-05, -2.0141436e-03],\n",
       "        dtype=float32)},\n",
       " 'marital_status': {' Divorced': array([-0.04537221,  0.03922546,  0.00435914, -0.01090962, -0.07099906,\n",
       "         -0.03937326, -0.00262114, -0.02010111, -0.00482285,  0.04258874,\n",
       "          0.02964552, -0.05108994,  0.03256331, -0.02029432,  0.03087497,\n",
       "         -0.05681242], dtype=float32),\n",
       "  ' Married-AF-spouse': array([ 0.00340489, -0.01289752,  0.01689666, -0.02089775,  0.01014925,\n",
       "         -0.02788256,  0.05299503,  0.04916327,  0.04270919, -0.04594683,\n",
       "         -0.06613699, -0.00906564, -0.01003866, -0.00382119,  0.0526241 ,\n",
       "         -0.0010448 ], dtype=float32),\n",
       "  ' Married-civ-spouse': array([-0.00060958, -0.06365891, -0.00800804,  0.04201369, -0.00047458,\n",
       "         -0.02228322,  0.04151668, -0.00597638,  0.01954204, -0.00772962,\n",
       "         -0.02206918,  0.05002231, -0.00369638, -0.02271152, -0.01655405,\n",
       "         -0.00837525], dtype=float32),\n",
       "  ' Married-spouse-absent': array([ 0.02688494,  0.04856828, -0.00170944, -0.00549449, -0.06896558,\n",
       "          0.0450816 , -0.01995509,  0.03432729, -0.04673363,  0.02250876,\n",
       "          0.00728557, -0.03250944,  0.02640908, -0.03366893, -0.03125948,\n",
       "          0.0162868 ], dtype=float32),\n",
       "  ' Never-married': array([ 0.00205923,  0.06758901, -0.03138069, -0.03716923, -0.00311962,\n",
       "          0.04764259,  0.01505854,  0.00323651, -0.03458356,  0.00367902,\n",
       "         -0.03867898, -0.06277826, -0.02396243,  0.042484  , -0.02989113,\n",
       "         -0.02312523], dtype=float32),\n",
       "  ' Separated': array([ 0.01158318,  0.0307892 ,  0.03348576, -0.01527441, -0.0287732 ,\n",
       "          0.0558467 , -0.09250664, -0.04474542, -0.00252542,  0.014233  ,\n",
       "          0.00343086, -0.07101759,  0.00313125,  0.04108253, -0.03927325,\n",
       "          0.03370704], dtype=float32),\n",
       "  ' Widowed': array([-0.04041148,  0.04942625,  0.01959193, -0.04740028, -0.04226643,\n",
       "         -0.04056646, -0.02350044, -0.04145321, -0.0134737 ,  0.04194918,\n",
       "         -0.00580211, -0.04135282,  0.01897697,  0.01432579,  0.03182293,\n",
       "          0.03135389], dtype=float32)},\n",
       " 'occupation': {' ?': array([-0.06383757,  0.00757895,  0.02902253, -0.01029979,  0.02574053,\n",
       "          0.08421702,  0.03357828, -0.0536965 , -0.01808362, -0.05023278,\n",
       "         -0.01763198,  0.03584713,  0.03138044,  0.02088807,  0.02411414,\n",
       "         -0.03180515], dtype=float32),\n",
       "  ' Adm-clerical': array([-0.02013278, -0.02991668,  0.028066  , -0.01193602, -0.0668179 ,\n",
       "         -0.03561223,  0.00166669,  0.04764635, -0.01571294, -0.03609832,\n",
       "         -0.06185134,  0.04193827,  0.03055181,  0.0254413 ,  0.02181093,\n",
       "         -0.00443763], dtype=float32),\n",
       "  ' Armed-Forces': array([-0.0385428 ,  0.06398584,  0.03412488, -0.05461176,  0.07924634,\n",
       "          0.00142843,  0.00964251,  0.00204203, -0.05621094, -0.00029599,\n",
       "         -0.0127021 , -0.02720691, -0.04257977, -0.03071633, -0.07159241,\n",
       "          0.04139773], dtype=float32),\n",
       "  ' Craft-repair': array([-0.02147998, -0.01371553,  0.03299114, -0.03434126,  0.02806675,\n",
       "          0.00145502,  0.03627812, -0.01169692, -0.0485619 , -0.02253866,\n",
       "          0.02808161,  0.00524545,  0.0184121 ,  0.01564508, -0.0619734 ,\n",
       "          0.01457794], dtype=float32),\n",
       "  ' Exec-managerial': array([ 0.05758491,  0.00110335, -0.01742005, -0.03313054, -0.01069352,\n",
       "          0.01157632,  0.01397194,  0.03061812, -0.04320148,  0.05355694,\n",
       "          0.01829818, -0.02962557,  0.03147669,  0.03416377,  0.01682969,\n",
       "         -0.00502274], dtype=float32),\n",
       "  ' Farming-fishing': array([-0.02977138,  0.07970565, -0.01447505,  0.03549652, -0.00640601,\n",
       "          0.01352912, -0.03435744,  0.00856788, -0.00635894, -0.05701474,\n",
       "         -0.00055708, -0.00067425,  0.00106736, -0.0597292 , -0.04388928,\n",
       "          0.09532568], dtype=float32),\n",
       "  ' Handlers-cleaners': array([-0.02461887,  0.00995016,  0.0391379 ,  0.09106866,  0.01540259,\n",
       "          0.03376949,  0.00428307,  0.04883476, -0.02038617, -0.06775276,\n",
       "         -0.02660764, -0.03024131, -0.03982168, -0.01934038,  0.00873069,\n",
       "          0.0378367 ], dtype=float32),\n",
       "  ' Machine-op-inspct': array([ 0.04008763,  0.02055719, -0.00076038,  0.02158451,  0.01939341,\n",
       "          0.04309667, -0.05466254, -0.01141183,  0.02451108, -0.03304178,\n",
       "          0.0209226 ,  0.03798034,  0.02200237, -0.05158177, -0.01400939,\n",
       "          0.03197689], dtype=float32),\n",
       "  ' Other-service': array([-0.07513345,  0.05805438,  0.0285818 , -0.01531995,  0.01306287,\n",
       "          0.01370454, -0.04492512,  0.05824165,  0.04679657, -0.00852699,\n",
       "         -0.04940678, -0.04354614, -0.02538597, -0.04748439, -0.04778553,\n",
       "         -0.03017446], dtype=float32),\n",
       "  ' Priv-house-serv': array([-0.05020012,  0.04853699, -0.00305813, -0.00330366, -0.00830333,\n",
       "          0.03362814, -0.02474731, -0.02765208, -0.00867978, -0.05868594,\n",
       "         -0.02196249, -0.03442717,  0.01824816, -0.06101691, -0.04895508,\n",
       "          0.0670489 ], dtype=float32),\n",
       "  ' Prof-specialty': array([ 0.02567154,  0.0330936 , -0.03070275, -0.01731768, -0.03971271,\n",
       "          0.033326  ,  0.02842152, -0.01651382,  0.00861667, -0.0240633 ,\n",
       "          0.06627692, -0.01249911, -0.01337055,  0.01745641,  0.04449829,\n",
       "         -0.02398681], dtype=float32),\n",
       "  ' Protective-serv': array([ 0.01778833, -0.0366494 ,  0.01888183, -0.0157674 ,  0.00827189,\n",
       "          0.03710321,  0.03656698, -0.06152011, -0.0266727 ,  0.03057458,\n",
       "         -0.00506678, -0.04251871,  0.0378149 ,  0.02812134,  0.02484998,\n",
       "         -0.05045823], dtype=float32),\n",
       "  ' Sales': array([ 0.01126879, -0.05240786,  0.00881608, -0.0434706 , -0.02630632,\n",
       "          0.00605325, -0.00273077, -0.00462225, -0.00558717,  0.01562254,\n",
       "         -0.0216125 ,  0.04732642,  0.01514684,  0.01211286, -0.01099923,\n",
       "          0.01660475], dtype=float32),\n",
       "  ' Tech-support': array([ 0.04897736, -0.04308388,  0.018674  , -0.0517262 ,  0.00252465,\n",
       "          0.00789054, -0.00156842,  0.03125786,  0.0272004 ,  0.03783512,\n",
       "          0.01953202,  0.02157349, -0.0189329 ,  0.00099308,  0.05555553,\n",
       "         -0.04714577], dtype=float32),\n",
       "  ' Transport-moving': array([ 0.03336184,  0.00737816, -0.00600733, -0.00993296,  0.046725  ,\n",
       "         -0.02543708,  0.03307518, -0.00984461, -0.01707754, -0.02254581,\n",
       "         -0.04123025,  0.03915344,  0.04439059, -0.01525223, -0.00110988,\n",
       "          0.01501463], dtype=float32)},\n",
       " 'relationship': {' Husband': array([ 0.013562  ,  0.0249253 ,  0.04391972, -0.00693502,  0.04482059,\n",
       "         -0.02518558,  0.01463263,  0.01900315, -0.01029495,  0.03030949,\n",
       "          0.04366489,  0.01548844, -0.03256852,  0.02928504, -0.01403544,\n",
       "          0.02265799], dtype=float32),\n",
       "  ' Not-in-family': array([ 0.03288025, -0.02088947,  0.04784498,  0.00802035,  0.02727966,\n",
       "          0.01221319,  0.01420794, -0.04355427, -0.01408206,  0.03208039,\n",
       "         -0.02096434, -0.01202983, -0.03351652, -0.02633545, -0.02770429,\n",
       "         -0.05700022], dtype=float32),\n",
       "  ' Other-relative': array([ 0.00165435,  0.03842173, -0.00053018, -0.03267672, -0.0159871 ,\n",
       "          0.02000366, -0.02599825,  0.02959968,  0.02673421,  0.03020395,\n",
       "         -0.08061741, -0.02574018,  0.03501547, -0.0392538 ,  0.01707573,\n",
       "          0.0433291 ], dtype=float32),\n",
       "  ' Own-child': array([-0.05581573,  0.03700284,  0.00603125, -0.03447749, -0.04028093,\n",
       "          0.06845186, -0.04504438,  0.08160713, -0.02811701,  0.05165605,\n",
       "          0.01342361, -0.02855146,  0.05096276,  0.00310146,  0.01687257,\n",
       "         -0.04453808], dtype=float32),\n",
       "  ' Unmarried': array([ 0.04107534,  0.00802099,  0.00724173, -0.00145837,  0.01047331,\n",
       "          0.05717226,  0.01688452, -0.02143315, -0.01061373, -0.0190458 ,\n",
       "          0.02840824,  0.00483262,  0.05270237,  0.02214681, -0.05875715,\n",
       "         -0.02145896], dtype=float32),\n",
       "  ' Wife': array([-0.06066239,  0.05572998,  0.02369455,  0.04665554,  0.03531222,\n",
       "         -0.03855142,  0.03052656, -0.02137484,  0.04280223, -0.04080893,\n",
       "          0.01342707, -0.03051473, -0.02109929, -0.00453467,  0.04299973,\n",
       "         -0.02947533], dtype=float32)},\n",
       " 'race': {' Amer-Indian-Eskimo': array([-0.06592835, -0.02411146, -0.03472224,  0.03499405,  0.04908062,\n",
       "          0.02520333,  0.00763246, -0.0501875 ,  0.02643171, -0.04024567,\n",
       "         -0.0609527 ,  0.03421284,  0.02745207, -0.08053725,  0.02773933,\n",
       "          0.00184795], dtype=float32),\n",
       "  ' Asian-Pac-Islander': array([ 0.02732968,  0.02182129,  0.012368  , -0.04350628,  0.02513515,\n",
       "          0.05423279, -0.01002284,  0.04121086, -0.05892651, -0.02238883,\n",
       "          0.03951342,  0.02401776, -0.00374165,  0.06068681, -0.04189891,\n",
       "          0.06429248], dtype=float32),\n",
       "  ' Black': array([ 0.02111569,  0.03024591,  0.04670143,  0.03087857, -0.02122835,\n",
       "         -0.0148301 ,  0.01112289,  0.04020369,  0.01142414,  0.003414  ,\n",
       "         -0.03799729,  0.03676658, -0.06927532,  0.0236964 , -0.0419075 ,\n",
       "          0.00738486], dtype=float32),\n",
       "  ' Other': array([ 0.01064833,  0.00843625, -0.0345051 ,  0.00746247,  0.03044013,\n",
       "         -0.03258761,  0.02198686, -0.03827816,  0.04038926, -0.02122057,\n",
       "          0.02068606, -0.04976099, -0.07377354,  0.02393284,  0.03800913,\n",
       "         -0.02282288], dtype=float32),\n",
       "  ' White': array([ 0.00014178, -0.03441684, -0.02273834, -0.02740277, -0.03417592,\n",
       "         -0.04096404,  0.02425107, -0.00422847,  0.02456893,  0.02994704,\n",
       "          0.01208864,  0.01336657,  0.04127183,  0.01456043,  0.03905572,\n",
       "          0.00653514], dtype=float32)},\n",
       " 'gender': {' Female': array([-0.01846912, -0.0464482 , -0.0342063 ,  0.00233257, -0.01150212,\n",
       "          0.04130113, -0.04098698, -0.00329003,  0.00607029, -0.0004267 ,\n",
       "         -0.07588667, -0.00589989, -0.05768478, -0.03967935,  0.00146054,\n",
       "          0.01371929], dtype=float32),\n",
       "  ' Male': array([-0.01666181,  0.02020943, -0.00096237,  0.00059564,  0.03393883,\n",
       "         -0.04496852,  0.01528664, -0.0319873 ,  0.00256209, -0.0568986 ,\n",
       "          0.06127226, -0.01167452, -0.0225115 , -0.0161063 , -0.00416945,\n",
       "          0.04785661], dtype=float32)},\n",
       " 'native_country': {' ?': array([-0.0201579 ,  0.06605973, -0.00956368,  0.050362  , -0.01148157,\n",
       "         -0.01573789,  0.0330676 ,  0.00829672,  0.03508273, -0.03655718,\n",
       "          0.00753393,  0.01481961, -0.01466836,  0.03588992, -0.03897341,\n",
       "         -0.02862521], dtype=float32),\n",
       "  ' Cambodia': array([-0.08446771, -0.01036854, -0.03209952,  0.0278476 ,  0.04667695,\n",
       "         -0.03376509, -0.02031144, -0.05928339, -0.03941671,  0.03619405,\n",
       "          0.0190925 ,  0.01547554,  0.03558858, -0.01352748,  0.00904786,\n",
       "         -0.01683228], dtype=float32),\n",
       "  ' Canada': array([-0.05459917,  0.02740512,  0.02180977, -0.03047813,  0.02571913,\n",
       "          0.00898254,  0.0034186 , -0.00457426,  0.00387938,  0.05306102,\n",
       "         -0.03645087, -0.02374595, -0.01739144,  0.02529542,  0.06382836,\n",
       "          0.02783794], dtype=float32),\n",
       "  ' China': array([ 0.02932892, -0.03669383, -0.042583  ,  0.01031701,  0.03298932,\n",
       "          0.03584559,  0.01667623, -0.01344429,  0.06634155, -0.0061873 ,\n",
       "         -0.04000005,  0.03141434, -0.01792469, -0.03210235, -0.00997709,\n",
       "          0.00278698], dtype=float32),\n",
       "  ' Columbia': array([ 0.0055302 , -0.00983494, -0.03024273, -0.0228174 ,  0.03869989,\n",
       "         -0.01038806,  0.06336614,  0.07721494, -0.01610615, -0.00281006,\n",
       "         -0.03588381,  0.03982366,  0.02594131,  0.05782818, -0.03334147,\n",
       "         -0.02173643], dtype=float32),\n",
       "  ' Cuba': array([-0.00414057,  0.0039052 ,  0.01961309, -0.00570975,  0.01915964,\n",
       "          0.00868004, -0.03477587, -0.02769563, -0.04928372,  0.02708222,\n",
       "          0.04420671, -0.01430688, -0.0071453 ,  0.00300181, -0.02876904,\n",
       "          0.00963014], dtype=float32),\n",
       "  ' Dominican-Republic': array([-0.02841367,  0.00408474,  0.02124184, -0.05378463,  0.05190651,\n",
       "          0.02975425, -0.04137857,  0.03375016, -0.03697469, -0.05720038,\n",
       "         -0.01676946, -0.00963546, -0.00096705,  0.00925803, -0.06571518,\n",
       "         -0.00434654], dtype=float32),\n",
       "  ' Ecuador': array([ 0.03076885,  0.00694529,  0.01725755, -0.03726745,  0.05046051,\n",
       "          0.06079291, -0.00583433, -0.02095902, -0.0104444 ,  0.00608161,\n",
       "         -0.02335666,  0.05938709,  0.01745896,  0.04953488, -0.00950069,\n",
       "          0.04752979], dtype=float32),\n",
       "  ' El-Salvador': array([ 0.0679374 , -0.04173823, -0.01032571, -0.00377671, -0.0402502 ,\n",
       "          0.02578722, -0.03648402,  0.05512004,  0.03460069,  0.0233472 ,\n",
       "         -0.02534575,  0.02365589,  0.03297972,  0.05104483, -0.02077545,\n",
       "          0.01218922], dtype=float32),\n",
       "  ' England': array([-0.0379862 ,  0.04880429, -0.0440743 ,  0.08528684,  0.01293024,\n",
       "          0.01045368,  0.03242157, -0.04379251, -0.00943262,  0.00339659,\n",
       "          0.02171146, -0.0030767 ,  0.00399405, -0.03682279,  0.0150565 ,\n",
       "         -0.02344106], dtype=float32),\n",
       "  ' France': array([ 0.00502283,  0.00128554,  0.02567463, -0.05810833, -0.0510952 ,\n",
       "         -0.0347181 ,  0.02516241,  0.0171742 , -0.03631672,  0.03436128,\n",
       "          0.02674914,  0.00038156,  0.06736507, -0.03644355,  0.001325  ,\n",
       "          0.02914901], dtype=float32),\n",
       "  ' Germany': array([-0.02623326, -0.0141681 ,  0.03343087,  0.03139154, -0.03807224,\n",
       "          0.03711917, -0.02341422, -0.00120982,  0.02667766,  0.05993627,\n",
       "         -0.03232827, -0.05109296, -0.003409  ,  0.00955248, -0.00119707,\n",
       "          0.04673352], dtype=float32),\n",
       "  ' Greece': array([ 0.05221988, -0.02710143,  0.01556736,  0.00239467,  0.03490807,\n",
       "         -0.02104293, -0.01355506,  0.0319602 ,  0.0260064 , -0.00739236,\n",
       "          0.02200395,  0.00835792,  0.01019681, -0.0315776 , -0.03758555,\n",
       "          0.03691999], dtype=float32),\n",
       "  ' Guatemala': array([ 0.02108199,  0.03200025, -0.00293209, -0.09058721,  0.01321175,\n",
       "          0.05880211,  0.06513727, -0.02567304,  0.02254778, -0.03519235,\n",
       "         -0.01508597, -0.01684925, -0.02880383, -0.02517832, -0.05308991,\n",
       "          0.00797211], dtype=float32),\n",
       "  ' Haiti': array([-0.00794983,  0.02505272, -0.02948636,  0.02903389,  0.0008383 ,\n",
       "         -0.05298638, -0.03398392,  0.02082856, -0.02738471,  0.01335407,\n",
       "          0.01676449,  0.01078872, -0.06919587, -0.05789374, -0.02472421,\n",
       "         -0.01847713], dtype=float32),\n",
       "  ' Holand-Netherlands': array([ 0.03045336, -0.00052301, -0.04860956, -0.00822856,  0.01862181,\n",
       "         -0.02798685,  0.02613243,  0.04883596,  0.0129864 ,  0.01532407,\n",
       "          0.01579895,  0.02370412, -0.04120988,  0.01005812, -0.0143601 ,\n",
       "         -0.03269187], dtype=float32),\n",
       "  ' Honduras': array([ 0.07131004,  0.0493837 , -0.06559678,  0.01565712, -0.01143765,\n",
       "          0.00523785, -0.0118175 , -0.01361672,  0.01499792,  0.05110962,\n",
       "         -0.06847119,  0.04953418,  0.02545198,  0.0303568 , -0.00933016,\n",
       "         -0.02673454], dtype=float32),\n",
       "  ' Hong': array([-0.00456467,  0.03184441, -0.0006785 ,  0.00444238,  0.02464365,\n",
       "          0.0274524 ,  0.01295936, -0.0495554 ,  0.00119165,  0.01730539,\n",
       "          0.0388713 , -0.0752503 , -0.02321769,  0.04688196, -0.03838965,\n",
       "         -0.02724035], dtype=float32),\n",
       "  ' Hungary': array([ 0.03136734,  0.03023218, -0.02461203, -0.05529936, -0.00124034,\n",
       "          0.02812957, -0.01149991, -0.03927097,  0.03785954,  0.06033598,\n",
       "          0.05030495, -0.0296477 , -0.01308279, -0.00407459,  0.02060359,\n",
       "         -0.05115419], dtype=float32),\n",
       "  ' India': array([ 0.0122944 ,  0.05693485, -0.05518712,  0.00537201, -0.01315318,\n",
       "         -0.00952488,  0.05103975, -0.02361989, -0.08709087, -0.02583128,\n",
       "         -0.01325317,  0.00849862,  0.01080109,  0.02255434,  0.03489433,\n",
       "          0.02953706], dtype=float32),\n",
       "  ' Iran': array([ 0.00566608,  0.00107415, -0.0276437 , -0.0330006 , -0.01200211,\n",
       "          0.09369078, -0.01377402, -0.04569036, -0.02880309, -0.03236404,\n",
       "          0.0361081 ,  0.01986472,  0.01506609,  0.02720379, -0.00074975,\n",
       "          0.02667174], dtype=float32),\n",
       "  ' Ireland': array([ 0.03530565,  0.02919136,  0.04265702,  0.0097084 ,  0.01176969,\n",
       "         -0.023022  ,  0.01895237,  0.04204193,  0.03895351,  0.01079978,\n",
       "         -0.0215836 , -0.0367269 , -0.05772172, -0.03366808, -0.0109884 ,\n",
       "          0.01233661], dtype=float32),\n",
       "  ' Italy': array([ 0.01135113,  0.03966015,  0.06398228, -0.01283999, -0.02083109,\n",
       "          0.03049222, -0.03309281, -0.01546708,  0.02721112,  0.00985586,\n",
       "          0.05608054, -0.02842323, -0.03522027, -0.01197131,  0.01831748,\n",
       "         -0.05789868], dtype=float32),\n",
       "  ' Jamaica': array([-0.02541961, -0.00567396,  0.02488687,  0.0020766 ,  0.03305579,\n",
       "         -0.04749065,  0.01902133, -0.0401081 ,  0.04664392,  0.00410469,\n",
       "         -0.00863641, -0.01728038, -0.02253678,  0.03343979,  0.00089859,\n",
       "         -0.02693372], dtype=float32),\n",
       "  ' Japan': array([ 0.07482451,  0.05133029, -0.00977686, -0.00582074,  0.02554625,\n",
       "          0.0021872 ,  0.01422135,  0.01389349, -0.04373727,  0.00277858,\n",
       "         -0.03091419, -0.06123691, -0.01737633, -0.01802327,  0.00648037,\n",
       "          0.02258971], dtype=float32),\n",
       "  ' Laos': array([-0.03230606, -0.07582698,  0.00558564,  0.00293621,  0.04247182,\n",
       "          0.08402192,  0.00511386,  0.019029  , -0.03708274, -0.0577633 ,\n",
       "         -0.01187369,  0.02926195, -0.02848902, -0.00023974,  0.00162869,\n",
       "         -0.04964351], dtype=float32),\n",
       "  ' Mexico': array([-0.01925249, -0.01122617, -0.00722364,  0.03518657,  0.02207869,\n",
       "         -0.05458038, -0.01904567,  0.05217757,  0.07237671,  0.00493503,\n",
       "         -0.05805502,  0.03720698,  0.04768106,  0.00692139, -0.05392677,\n",
       "         -0.00383396], dtype=float32),\n",
       "  ' Nicaragua': array([-0.02907041, -0.03937788, -0.03351875,  0.01119202, -0.02693949,\n",
       "         -0.0179015 ,  0.02584763,  0.03176047,  0.0218387 , -0.01937638,\n",
       "          0.00746083,  0.00711799,  0.03557328, -0.01847506, -0.06276499,\n",
       "          0.00361122], dtype=float32),\n",
       "  ' Outlying-US(Guam-USVI-etc)': array([-0.02180033, -0.00526853, -0.03228227, -0.02582512,  0.01042712,\n",
       "          0.03022241, -0.0411585 ,  0.0643267 , -0.0108421 , -0.00555868,\n",
       "         -0.01343014,  0.01754645,  0.05117338,  0.02947392,  0.02756113,\n",
       "         -0.04565253], dtype=float32),\n",
       "  ' Peru': array([ 0.04478522, -0.06470728, -0.0009027 , -0.00699389, -0.0584656 ,\n",
       "         -0.03492106, -0.01991409, -0.04774018,  0.00047848,  0.02422306,\n",
       "          0.01513162,  0.05469042,  0.05287505,  0.03286216, -0.02981912,\n",
       "         -0.00333064], dtype=float32),\n",
       "  ' Philippines': array([-0.04348605,  0.01213721,  0.02055076,  0.02150409,  0.01368526,\n",
       "         -0.03411115,  0.05370921, -0.05321322,  0.01172798, -0.03064973,\n",
       "          0.03924683, -0.00321236, -0.0307527 ,  0.02442019,  0.0189982 ,\n",
       "          0.00835769], dtype=float32),\n",
       "  ' Poland': array([ 0.00968725,  0.07355418,  0.01315026, -0.01273458,  0.02061559,\n",
       "         -0.02931318,  0.03557031,  0.01814875, -0.04569579,  0.01706666,\n",
       "          0.00335477,  0.01388597,  0.01289967, -0.01992559, -0.02077043,\n",
       "          0.05109793], dtype=float32),\n",
       "  ' Portugal': array([ 0.0115776 , -0.01550853, -0.01442243,  0.02371118, -0.00236547,\n",
       "          0.03563911,  0.01626235,  0.02173354, -0.05123286,  0.00386649,\n",
       "         -0.02234758, -0.03848999,  0.06142095,  0.02565203, -0.03753582,\n",
       "         -0.00435256], dtype=float32),\n",
       "  ' Puerto-Rico': array([ 0.05654891, -0.01446602, -0.03219929,  0.00139227,  0.03869524,\n",
       "         -0.02627621,  0.01867837, -0.02935226,  0.03621568, -0.02794887,\n",
       "         -0.03163141,  0.05418199, -0.01000187,  0.00790675, -0.00390756,\n",
       "          0.00638582], dtype=float32),\n",
       "  ' Scotland': array([ 6.7635924e-03, -8.5066138e-03,  1.9183442e-02,  3.7333851e-03,\n",
       "          1.5184076e-02,  7.4070806e-05,  8.6668860e-03,  3.8622219e-02,\n",
       "          4.0452266e-03, -3.1510074e-02,  2.7332161e-02,  8.1593636e-03,\n",
       "         -2.2765301e-02, -2.4444854e-02, -1.2715766e-02,  2.2562634e-02],\n",
       "        dtype=float32),\n",
       "  ' South': array([ 0.00055416,  0.00347826, -0.02663793,  0.00046058, -0.00511317,\n",
       "          0.0412039 ,  0.01183994,  0.02967525,  0.02886489, -0.00805535,\n",
       "         -0.03803416,  0.04070604,  0.00583337,  0.00383502,  0.0384391 ,\n",
       "         -0.06521309], dtype=float32),\n",
       "  ' Taiwan': array([-0.04029812, -0.03447414,  0.0607441 ,  0.01178467, -0.04849945,\n",
       "          0.05918084,  0.01660522, -0.01158092,  0.00727667, -0.01307476,\n",
       "          0.00996281,  0.03067523,  0.03308541,  0.03755815,  0.03061291,\n",
       "          0.02522198], dtype=float32),\n",
       "  ' Thailand': array([ 0.07265999, -0.0029206 ,  0.04365681,  0.03830242,  0.01673051,\n",
       "          0.01556376, -0.01156573, -0.02697114,  0.04023357, -0.01980873,\n",
       "          0.01573669, -0.00054856, -0.04762943,  0.03030788,  0.01105164,\n",
       "         -0.01999931], dtype=float32),\n",
       "  ' Trinadad&Tobago': array([-0.01178518, -0.01165492,  0.01840627,  0.05281764,  0.02816287,\n",
       "         -0.00277499, -0.02040106,  0.00717029,  0.04382868,  0.0091321 ,\n",
       "         -0.0758718 ,  0.00845573, -0.06480145,  0.01442702, -0.04496184,\n",
       "         -0.07831459], dtype=float32),\n",
       "  ' United-States': array([-1.6290952e-02, -4.4092424e-03, -4.0335945e-06,  3.4229111e-02,\n",
       "         -2.0806028e-02,  2.3357077e-02, -5.2393790e-02,  9.9837519e-03,\n",
       "         -3.1242723e-02,  1.5649782e-02,  2.6398329e-02, -1.8780254e-02,\n",
       "          1.7798880e-02, -4.4789445e-02,  3.0238144e-02,  1.7265886e-02],\n",
       "        dtype=float32),\n",
       "  ' Vietnam': array([-0.04605067,  0.02874081, -0.0070491 , -0.00480294,  0.03355922,\n",
       "          0.02097244, -0.03930542,  0.04611671,  0.05237558, -0.0030575 ,\n",
       "         -0.10108037,  0.0690866 , -0.04474528,  0.02569848, -0.00622951,\n",
       "          0.01898586], dtype=float32),\n",
       "  ' Yugoslavia': array([-9.99166351e-03,  1.88647192e-02,  2.58885100e-02,  7.22031221e-02,\n",
       "          4.28112596e-02, -6.34874741e-05, -5.93920946e-02,  1.76756755e-02,\n",
       "          2.04394963e-02, -1.38157355e-02, -4.42298986e-02, -2.08621081e-02,\n",
       "          1.53022371e-02, -5.18672243e-02,  2.83514298e-02, -2.21115425e-02],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = {}\n",
    "\n",
    "for layer in model.layers: \n",
    "    if \"_embedding\" in  layer.get_config()[\"name\"]:\n",
    "        col_name = layer.get_config()[\"name\"].split(\"_embedding\")[0]\n",
    "        if col_name not in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "            continue\n",
    "        embeddings[col_name] = {}\n",
    "        for idx, cat in enumerate(CATEGORICAL_FEATURES_WITH_VOCABULARY[col_name]):\n",
    "            embeddings[col_name][cat] = layer.get_weights()[0][idx]\n",
    "            \n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc9cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3573e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
