{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190e8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e78d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x17826be9840>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.device(\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bacd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16281, 15)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Prepare the data\n",
    "This example uses the\n",
    "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
    "provided by the\n",
    "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "The task is binary classification\n",
    "to predict whether a person is likely to be making over USD 50,000 a year.\n",
    "The dataset includes 48,842 instances with 14 input features: 5 numerical features and 9 categorical features.\n",
    "First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n",
    "DataFrame:\n",
    "\"\"\"\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "\"\"\"\n",
    "Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels.\n",
    "\"\"\"\n",
    "# n = len(train_data)\n",
    "# train_data = train_data.sample(int(n/12)).reset_index(drop=True).copy()\n",
    "\n",
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")\n",
    "\n",
    "\"\"\"\n",
    "Now we store the training and test data in separate CSV files.\n",
    "\"\"\"\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)\n",
    "\n",
    "\"\"\"\n",
    "## Define dataset metadata\n",
    "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "the data into input features, and encoding the input features with respect to their types.\n",
    "\"\"\"\n",
    "\n",
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f32a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\singl\\Anaconda3\\envs\\tabtrabsfomer\\lib\\site-packages\\numpy\\core\\numeric.py:2468: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "The hyperparameters includes model architecture and training configurations.\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 8  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "\"\"\"\n",
    "## Implement data reading pipeline\n",
    "We define an input function that reads and parses the file, then converts features\n",
    "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "for training or evaluation.\n",
    "\"\"\"\n",
    "\n",
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Implement a training and evaluation procedure\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694b97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create model inputs\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=1,\n",
    "                output_mode=\"int\",\n",
    "                name = feature_name+\"_string_lookup\"\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])-1\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims,\n",
    "                name=feature_name+\"_embedding\"\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement an MLP block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f42d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(model, filename):\n",
    "    embeddings = {}\n",
    "\n",
    "    for layer in model.layers: \n",
    "        if \"_embedding\" in  layer.get_config()[\"name\"]:\n",
    "            col_name = layer.get_config()[\"name\"].split(\"_embedding\")[0]\n",
    "            if col_name not in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "                continue\n",
    "            embeddings[col_name] = {}\n",
    "            for idx, cat in enumerate(CATEGORICAL_FEATURES_WITH_VOCABULARY[col_name]):\n",
    "                if \"mask\" in cat:\n",
    "                    continue\n",
    "                embeddings[col_name][cat] = layer.get_weights()[0][idx]\n",
    "            \n",
    "    with open(filename, 'wb') as config_dictionary_file:\n",
    "        pickle.dump(embeddings, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc13376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model, filename):\n",
    "    with open(filename, 'rb') as config_dictionary_file:\n",
    "        embeddings = pickle.load(config_dictionary_file)\n",
    "    \n",
    "    for layer in model.layers: \n",
    "        if \"_embedding\" in  layer.get_config()[\"name\"]:\n",
    "            col_name = layer.get_config()[\"name\"].split(\"_embedding\")[0]\n",
    "            if col_name not in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "                continue\n",
    "            layer.set_weights([np.array(list([embeddings[col_name][c] \\\n",
    "                                             for c in CATEGORICAL_FEATURES_WITH_VOCABULARY[col_name]]))])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6e7d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/10\n",
      "    123/Unknown - 10s 23ms/step - loss: 80142.0156 - accuracy: 0.7927WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 13s 45ms/step - loss: 80142.0156 - accuracy: 0.7927 - val_loss: 66135.3047 - val_accuracy: 0.8436\n",
      "Epoch 2/10\n",
      "123/123 [==============================] - ETA: 0s - loss: 68602.7891 - accuracy: 0.8249WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 68602.7891 - accuracy: 0.8249 - val_loss: 64041.6523 - val_accuracy: 0.8456\n",
      "Epoch 3/10\n",
      "123/123 [==============================] - ETA: 0s - loss: 66280.6016 - accuracy: 0.8321WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 66280.6016 - accuracy: 0.8321 - val_loss: 62326.8906 - val_accuracy: 0.8436\n",
      "Epoch 4/10\n",
      "122/123 [============================>.] - ETA: 0s - loss: 64645.2852 - accuracy: 0.8348WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 64799.1992 - accuracy: 0.8346 - val_loss: 61897.0234 - val_accuracy: 0.8455\n",
      "Epoch 5/10\n",
      "123/123 [==============================] - ETA: 0s - loss: 64543.8320 - accuracy: 0.8375WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 64543.8320 - accuracy: 0.8375 - val_loss: 62382.9062 - val_accuracy: 0.8386\n",
      "Epoch 6/10\n",
      "121/123 [============================>.] - ETA: 0s - loss: 63256.3477 - accuracy: 0.8395WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 27ms/step - loss: 63408.4961 - accuracy: 0.8392 - val_loss: 62497.3242 - val_accuracy: 0.8445\n",
      "Epoch 7/10\n",
      "122/123 [============================>.] - ETA: 0s - loss: 62861.3242 - accuracy: 0.8395WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 62991.4688 - accuracy: 0.8391 - val_loss: 61786.9883 - val_accuracy: 0.8463\n",
      "Epoch 8/10\n",
      "122/123 [============================>.] - ETA: 0s - loss: 62616.4648 - accuracy: 0.8408WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 26ms/step - loss: 62740.9180 - accuracy: 0.8405 - val_loss: 62586.4805 - val_accuracy: 0.8439\n",
      "Epoch 9/10\n",
      "121/123 [============================>.] - ETA: 0s - loss: 62099.5625 - accuracy: 0.8419WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 20ms/step - loss: 62229.7891 - accuracy: 0.8416 - val_loss: 61519.0000 - val_accuracy: 0.8452\n",
      "Epoch 10/10\n",
      "122/123 [============================>.] - ETA: 0s - loss: 62286.3516 - accuracy: 0.8408WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 2s 20ms/step - loss: 62415.6328 - accuracy: 0.8404 - val_loss: 62279.4688 - val_accuracy: 0.8441\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "Validation accuracy: 84.41%\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten(name=\"dyanmic_embedding\")(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "tabtransformer_model = load_embeddings(tabtransformer_model, 'unsupervised_trained_embeddings.dictionary')\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the TabTransformer model:\n",
    "\"\"\"\n",
    "\n",
    "history, tabtransformer_model = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edacced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(tabtransformer_model, 'semi_supervised_trained_embeddings.dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7280ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
