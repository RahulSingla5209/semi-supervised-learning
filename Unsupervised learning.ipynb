{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190e8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567e78d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x24f70263f80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.device(\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2399849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Prepare the data\n",
    "This example uses the\n",
    "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/census+income)\n",
    "provided by the\n",
    "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "The task is binary classification\n",
    "to predict whether a person is likely to be making over USD 50,000 a year.\n",
    "The dataset includes 48,842 instances with 14 input features: 5 numerical features and 9 categorical features.\n",
    "First, let's load the dataset from the UCI Machine Learning Repository into a Pandas\n",
    "DataFrame:\n",
    "\"\"\"\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "\n",
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"native_country\",\n",
    "]\n",
    "\n",
    "train_data.drop(columns = NUMERIC_FEATURE_NAMES, inplace=True)\n",
    "train_data.drop(columns = \"income_bracket\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5460e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token count 102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[100,   9,  22,  68,  88,  86,  17,  64],\n",
       "       [ 99,   9,  20,  71,  87,  86,  17,  64],\n",
       "       [ 97,  11,  18,  73,  88,  86,  17,  64],\n",
       "       [ 97,   1,  20,  73,  87,  84,  17,  64],\n",
       "       [ 97,   9,  20,  77,  92,  84,  16,  30]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "all_data = []\n",
    "for f in CATEGORICAL_FEATURE_NAMES:\n",
    "    all_data = all_data + train_data[f].apply(lambda x: f+x).values.tolist()\n",
    "    \n",
    "n = len(train_data)\n",
    "cat_data = le.fit_transform(all_data).reshape(len(CATEGORICAL_FEATURE_NAMES),n).T\n",
    "cat_count = len(np.unique(cat_data))\n",
    "print(\"Total token count\", cat_count)\n",
    "cat_data[:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621cc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_pos = random.randint(len(CATEGORICAL_FEATURE_NAMES), size=(n))\n",
    "masked_data = train_data[CATEGORICAL_FEATURE_NAMES].copy()\n",
    "mask_label = []\n",
    "for i in range(n):\n",
    "    mask_label = mask_label + [cat_data[i, masks_pos[i]]]\n",
    "    masked_data.iloc[i, masks_pos[i]] = \"mask\"+str(masks_pos[i])\n",
    "\n",
    "le2=LabelEncoder()\n",
    "mask_label = le2.fit_transform(mask_label)    \n",
    "masked_data[\"fnlwgt\"]=train_data[\"fnlwgt\"]\n",
    "train_data = masked_data\n",
    "masked_data[\"label\"] = mask_label\n",
    "CSV_HEADER = train_data.columns\n",
    "masked_data.head(10)\n",
    "\n",
    "\"\"\"\n",
    "Now we store the training and test data in separate CSV files.\n",
    "\"\"\"\n",
    "\n",
    "train_data_file = \"train_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abda2990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native_country</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>77516</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>mask3</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>83311</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private</td>\n",
       "      <td>mask1</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>215646</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mask0</td>\n",
       "      <td>11th</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>234721</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>mask2</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>338409</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>mask6</td>\n",
       "      <td>United-States</td>\n",
       "      <td>284582</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Private</td>\n",
       "      <td>mask1</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>160187</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>mask4</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>209642</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Private</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>mask4</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>45781</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>mask7</td>\n",
       "      <td>159449</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           workclass   education          marital_status          occupation  \\\n",
       "0          State-gov   Bachelors           Never-married               mask3   \n",
       "1   Self-emp-not-inc   Bachelors      Married-civ-spouse               mask3   \n",
       "2            Private       mask1                Divorced   Handlers-cleaners   \n",
       "3              mask0        11th      Married-civ-spouse   Handlers-cleaners   \n",
       "4            Private   Bachelors                   mask2      Prof-specialty   \n",
       "5            Private     Masters      Married-civ-spouse     Exec-managerial   \n",
       "6            Private       mask1   Married-spouse-absent       Other-service   \n",
       "7   Self-emp-not-inc     HS-grad      Married-civ-spouse     Exec-managerial   \n",
       "8            Private     Masters           Never-married      Prof-specialty   \n",
       "9            Private   Bachelors      Married-civ-spouse     Exec-managerial   \n",
       "\n",
       "     relationship    race   gender  native_country  fnlwgt  label  \n",
       "0   Not-in-family   White     Male   United-States   77516     64  \n",
       "1         Husband   White     Male   United-States   83311     67  \n",
       "2   Not-in-family   White     Male   United-States  215646     11  \n",
       "3         Husband   Black     Male   United-States  234721     92  \n",
       "4            Wife   Black   Female            Cuba  338409     20  \n",
       "5            Wife   White    mask6   United-States  284582     16  \n",
       "6   Not-in-family   Black   Female         Jamaica  160187      6  \n",
       "7           mask4   White     Male   United-States  209642     83  \n",
       "8           mask4   White   Female   United-States   45781     84  \n",
       "9         Husband   White     Male           mask7  159449     60  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f294a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Define dataset metadata\n",
    "Here, we define the metadata of the dataset that will be useful for reading and parsing\n",
    "the data into input features, and encoding the input features with respect to their types.\n",
    "\"\"\"\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09bacd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all the input features.\n",
    "FEATURE_NAMES = CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"label\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = masked_data[\"label\"].unique()\n",
    "\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f32a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "The hyperparameters includes model architecture and training configurations.\n",
    "\"\"\"\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 1  # Number of transformer blocks.\n",
    "NUM_HEADS = 2  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 8  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model.\n",
    "\n",
    "\"\"\"\n",
    "## Implement data reading pipeline\n",
    "We define an input function that reads and parses the file, then converts features\n",
    "and labels into a[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets)\n",
    "for training or evaluation.\n",
    "\"\"\"\n",
    "\n",
    "# target_label_lookup = layers.StringLookup(\n",
    "#     vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    "# )\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    # target_index = target_label_lookup(target)\n",
    "    target = int(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        #column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=True,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f91c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Implement a training and evaluation procedure\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694b97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create model inputs\n",
    "Now, define the inputs for the models as a dictionary, where the key is the feature name,\n",
    "and the value is a `keras.layers.Input` tensor with the corresponding feature shape\n",
    "and data type.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Encode features\n",
    "The `encode_inputs` method returns `encoded_categorical_feature_list` and `numerical_feature_list`.\n",
    "We encode the categorical features as embeddings, using a fixed `embedding_dims` for all the features,\n",
    "regardless their vocabulary sizes. This is required for the Transformer model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        # Get the vocabulary of the categorical feature.\n",
    "        vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "        # Create a lookup to convert string values to an integer indices.\n",
    "        # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "        # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "        lookup = layers.StringLookup(\n",
    "            vocabulary=vocabulary,\n",
    "            mask_token=None,\n",
    "            num_oov_indices=1,\n",
    "            output_mode=\"int\",\n",
    "        )\n",
    "\n",
    "        # Convert the string input values into integer indices.\n",
    "        encoded_feature = lookup(inputs[feature_name])-1\n",
    "\n",
    "        # Create an embedding layer with the specified dimensions.\n",
    "        embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims,\n",
    "                name=feature_name+\"_embedding\"\n",
    "            )\n",
    "\n",
    "        # Convert the index values to embedding representations.\n",
    "        encoded_categorical_feature = embedding(encoded_feature)\n",
    "        encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement an MLP block\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6e7d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'workclass_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'education_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'marital_status_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'occupation_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'relationship_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'race_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'gender_embedding')>, <KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'native_country_embedding')>]\n",
      "Total model weights: 24705\n",
      "Start training the model...\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 5s 19ms/step - loss: 348572.7188 - accuracy: 0.5474\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 208197.8750 - accuracy: 0.6645\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 192193.5156 - accuracy: 0.6823\n",
      "Epoch 4/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 183572.3594 - accuracy: 0.6908\n",
      "Epoch 5/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 178830.9375 - accuracy: 0.6972\n",
      "Epoch 6/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 175725.0781 - accuracy: 0.6977\n",
      "Epoch 7/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 173371.4062 - accuracy: 0.7003\n",
      "Epoch 8/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 171714.3906 - accuracy: 0.7005\n",
      "Epoch 9/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 170261.1562 - accuracy: 0.7021\n",
      "Epoch 10/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 169520.4688 - accuracy: 0.7023\n",
      "Epoch 11/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 168524.5781 - accuracy: 0.7034\n",
      "Epoch 12/20\n",
      "123/123 [==============================] - 2s 13ms/step - loss: 167573.9688 - accuracy: 0.7054\n",
      "Epoch 13/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 166876.5938 - accuracy: 0.7049\n",
      "Epoch 14/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 166249.4375 - accuracy: 0.7044\n",
      "Epoch 15/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 166009.1562 - accuracy: 0.7055\n",
      "Epoch 16/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 165347.7656 - accuracy: 0.7058\n",
      "Epoch 17/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 164392.7188 - accuracy: 0.7068\n",
      "Epoch 18/20\n",
      "123/123 [==============================] - 2s 15ms/step - loss: 164340.3438 - accuracy: 0.7081\n",
      "Epoch 19/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 163718.1719 - accuracy: 0.7089\n",
      "Epoch 20/20\n",
      "123/123 [==============================] - 2s 14ms/step - loss: 163742.0156 - accuracy: 0.7084\n",
      "Model training finished\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    print(encoded_categorical_feature_list)\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten(name=\"dyanmic_embedding\")(encoded_categorical_features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * categorical_features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(categorical_features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=len(TARGET_LABELS), activation=\"softmax\", name=\"softmax\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "\"\"\"\n",
    "Let's train and evaluate the TabTransformer model:\n",
    "\"\"\"\n",
    "\n",
    "history, model = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23e8d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60    0.112558\n",
       "82    0.107153\n",
       "92    0.088910\n",
       "17    0.082430\n",
       "20    0.056233\n",
       "        ...   \n",
       "41    0.000061\n",
       "65    0.000061\n",
       "58    0.000061\n",
       "47    0.000061\n",
       "96    0.000031\n",
       "Name: label, Length: 97, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(masked_data[\"label\"]).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9231f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workclass': {' ?': array([ 0.0209779 , -0.01017089,  0.05762693, -0.02040695, -0.06742756,\n",
       "         -0.04062385,  0.08188439,  0.01881742], dtype=float32),\n",
       "  ' Federal-gov': array([-0.00238482,  0.02157347,  0.03422296,  0.03657641,  0.0712916 ,\n",
       "         -0.03283327, -0.02137609, -0.05952976], dtype=float32),\n",
       "  ' Local-gov': array([ 0.0074615 , -0.07071084, -0.00371071,  0.00851991, -0.02765904,\n",
       "          0.05101799, -0.00939276, -0.04100754], dtype=float32),\n",
       "  ' Never-worked': array([ 0.03243618,  0.04083101,  0.00466387, -0.01333358, -0.04312501,\n",
       "         -0.04420219,  0.07556188,  0.02350184], dtype=float32),\n",
       "  ' Private': array([ 0.07735993, -0.06224407, -0.06876317,  0.0360746 ,  0.03068237,\n",
       "         -0.02371984, -0.01185691,  0.02789628], dtype=float32),\n",
       "  ' Self-emp-inc': array([-0.02888541,  0.0220619 , -0.04513967, -0.07610089,  0.03344824,\n",
       "         -0.02240068, -0.01810209,  0.03336848], dtype=float32),\n",
       "  ' Self-emp-not-inc': array([ 0.00490372,  0.00060253, -0.10424689, -0.04242617,  0.0332447 ,\n",
       "          0.04423108,  0.05513534,  0.03723662], dtype=float32),\n",
       "  ' State-gov': array([-0.02101822,  0.01065599, -0.06900267,  0.06416547, -0.00868189,\n",
       "         -0.01128609, -0.00981953, -0.06398731], dtype=float32),\n",
       "  ' Without-pay': array([ 0.0067314 , -0.07771736,  0.08462879,  0.02287661,  0.03912644,\n",
       "         -0.00299505, -0.00621219,  0.01830693], dtype=float32)},\n",
       " 'education': {' 10th': array([ 0.01004055, -0.01484738, -0.00978454,  0.04143376,  0.03440506,\n",
       "         -0.00994342, -0.06284318, -0.01020395], dtype=float32),\n",
       "  ' 11th': array([ 0.04384557, -0.02319704, -0.02196386,  0.0340667 ,  0.02221416,\n",
       "          0.01133999, -0.08417749,  0.01402144], dtype=float32),\n",
       "  ' 12th': array([ 0.00993827, -0.07208956, -0.00385371,  0.01173527,  0.01604907,\n",
       "          0.05499323, -0.02086186, -0.01443927], dtype=float32),\n",
       "  ' 1st-4th': array([-0.03024182,  0.00822619, -0.0667112 ,  0.09758695,  0.02348472,\n",
       "         -0.00468709,  0.03252158,  0.02539724], dtype=float32),\n",
       "  ' 5th-6th': array([-0.03798443, -0.07734732, -0.06272665,  0.02323579, -0.03121571,\n",
       "         -0.00251894,  0.05255411,  0.05422169], dtype=float32),\n",
       "  ' 7th-8th': array([-0.03588395,  0.01274302, -0.05725466,  0.07181846, -0.00071831,\n",
       "          0.06704608, -0.02133272,  0.01798354], dtype=float32),\n",
       "  ' 9th': array([-0.03636463, -0.07524731, -0.01673565,  0.01782041, -0.06833677,\n",
       "          0.06452605,  0.0016237 ,  0.04401644], dtype=float32),\n",
       "  ' Assoc-acdm': array([-0.07255005, -0.01531959,  0.04872103, -0.03219155,  0.05050699,\n",
       "          0.05243483, -0.022155  , -0.01006501], dtype=float32),\n",
       "  ' Assoc-voc': array([-0.05655585, -0.02316   ,  0.04361975, -0.01782978,  0.06174708,\n",
       "          0.00293754, -0.12292008,  0.00192155], dtype=float32),\n",
       "  ' Bachelors': array([-0.01523283,  0.00056361,  0.09948341, -0.01766982,  0.08240014,\n",
       "          0.0032717 , -0.01898079, -0.03184718], dtype=float32),\n",
       "  ' Doctorate': array([ 0.00753218,  0.04622737,  0.03435467, -0.0993101 ,  0.0166855 ,\n",
       "         -0.06188583,  0.03500212,  0.05919968], dtype=float32),\n",
       "  ' HS-grad': array([-0.00710543,  0.00586608,  0.04757437,  0.04564251,  0.00334915,\n",
       "          0.06539297, -0.05726568, -0.01166844], dtype=float32),\n",
       "  ' Masters': array([ 0.00086583,  0.03361336,  0.03380722, -0.05163652,  0.08193764,\n",
       "         -0.03654602, -0.01950441, -0.01806881], dtype=float32),\n",
       "  ' Preschool': array([-0.08109289,  0.00378077, -0.02435344,  0.01211925,  0.03840789,\n",
       "         -0.0751925 ,  0.06406705,  0.07918721], dtype=float32),\n",
       "  ' Prof-school': array([-0.00582105,  0.07927988,  0.024917  , -0.10030441, -0.01412525,\n",
       "         -0.02249001, -0.01366773,  0.0554956 ], dtype=float32),\n",
       "  ' Some-college': array([ 0.01862534, -0.05278045,  0.09159458, -0.00141393, -0.00774963,\n",
       "          0.0325217 , -0.05509192,  0.00269558], dtype=float32)},\n",
       " 'marital_status': {' Divorced': array([-0.05119885, -0.0397043 ,  0.00168379,  0.06542084, -0.07396509,\n",
       "          0.07532649, -0.04000349, -0.00809976], dtype=float32),\n",
       "  ' Married-AF-spouse': array([ 0.0281703 , -0.07389274,  0.0337109 ,  0.00515292,  0.00187461,\n",
       "         -0.06835239,  0.07602107,  0.02641841], dtype=float32),\n",
       "  ' Married-civ-spouse': array([ 0.00776837, -0.08007015,  0.07016251, -0.01707052,  0.02686493,\n",
       "         -0.042876  ,  0.04210878,  0.00808295], dtype=float32),\n",
       "  ' Married-spouse-absent': array([-0.05187951,  0.04486663,  0.02683838,  0.03848002,  0.05885174,\n",
       "          0.08771892, -0.08708339,  0.02828006], dtype=float32),\n",
       "  ' Never-married': array([-0.01161883,  0.03151913,  0.07577575, -0.02238033, -0.0481008 ,\n",
       "          0.01631971, -0.0698264 , -0.03740195], dtype=float32),\n",
       "  ' Separated': array([-0.00577812,  0.03904491, -0.04517361,  0.04336029,  0.0171964 ,\n",
       "          0.04962612, -0.03546503, -0.02292425], dtype=float32),\n",
       "  ' Widowed': array([-0.00393742,  0.03016175, -0.03697508,  0.03221705, -0.09185167,\n",
       "          0.001043  , -0.02075844,  0.03344898], dtype=float32)},\n",
       " 'occupation': {' ?': array([-0.02466867,  0.02845761, -0.05122249, -0.03337767, -0.11627833,\n",
       "          0.04358701, -0.06515148,  0.0478467 ], dtype=float32),\n",
       "  ' Adm-clerical': array([ 0.00236129,  0.01364138, -0.02247471,  0.05990002,  0.0701066 ,\n",
       "          0.03838333, -0.01311467, -0.01106578], dtype=float32),\n",
       "  ' Armed-Forces': array([-0.03025118,  0.04516285,  0.01925014, -0.03065042,  0.05930917,\n",
       "         -0.03658155, -0.02998124,  0.04516781], dtype=float32),\n",
       "  ' Craft-repair': array([-0.01820687,  0.00638106,  0.02682203, -0.09361921,  0.07452326,\n",
       "         -0.03188077,  0.00630506, -0.01033125], dtype=float32),\n",
       "  ' Exec-managerial': array([-0.0349227 ,  0.00840138,  0.01757509,  0.03619153,  0.00231401,\n",
       "          0.01759284,  0.07226233, -0.07342622], dtype=float32),\n",
       "  ' Farming-fishing': array([-0.00168313,  0.0426819 ,  0.0361828 , -0.12452521,  0.00502406,\n",
       "          0.01040831,  0.04488897,  0.01783183], dtype=float32),\n",
       "  ' Handlers-cleaners': array([-0.04822309, -0.02533511,  0.0648912 , -0.05725752,  0.07818533,\n",
       "         -0.03650313, -0.01412675,  0.05625077], dtype=float32),\n",
       "  ' Machine-op-inspct': array([-0.06895684, -0.00944465,  0.02398009, -0.00657461,  0.08709822,\n",
       "         -0.01717512, -0.02761156,  0.02823092], dtype=float32),\n",
       "  ' Other-service': array([-0.03445696, -0.03421876, -0.0081097 , -0.00297516,  0.085099  ,\n",
       "          0.05617781, -0.00310715,  0.07236758], dtype=float32),\n",
       "  ' Priv-house-serv': array([-0.02027829, -0.05033941,  0.06237246,  0.0110932 ,  0.00414846,\n",
       "          0.06939257, -0.03451131,  0.06853464], dtype=float32),\n",
       "  ' Prof-specialty': array([ 0.05435394, -0.09851523,  0.03505537,  0.01016981, -0.03841875,\n",
       "          0.01849765,  0.00402522, -0.07523984], dtype=float32),\n",
       "  ' Protective-serv': array([ 0.09913316,  0.02323622, -0.04059349, -0.02565528,  0.06488517,\n",
       "         -0.03527224, -0.05691765, -0.01805486], dtype=float32),\n",
       "  ' Sales': array([-0.08695269, -0.00320433,  0.04381289,  0.03417928, -0.02251555,\n",
       "         -0.0110274 ,  0.0569844 ,  0.00250606], dtype=float32),\n",
       "  ' Tech-support': array([-0.00426381, -0.0354831 , -0.0081373 ,  0.03884602,  0.05373239,\n",
       "         -0.03129605, -0.00550568, -0.04906619], dtype=float32),\n",
       "  ' Transport-moving': array([-0.03929593,  0.00670231,  0.00037707, -0.09442089,  0.05663908,\n",
       "         -0.03186441, -0.00884545, -0.01392223], dtype=float32)},\n",
       " 'relationship': {' Husband': array([-0.03617693, -0.04844902, -0.01498154,  0.03224308, -0.06980682,\n",
       "          0.02293771, -0.02302376,  0.06944361], dtype=float32),\n",
       "  ' Not-in-family': array([ 0.0817508 , -0.06163291, -0.02744234,  0.04374737,  0.03455707,\n",
       "          0.00402713,  0.04793978, -0.05857285], dtype=float32),\n",
       "  ' Other-relative': array([ 0.0812807 , -0.01362318, -0.08597131,  0.0089865 , -0.00552688,\n",
       "         -0.0660954 ,  0.04013051,  0.07319046], dtype=float32),\n",
       "  ' Own-child': array([-7.4256235e-03,  1.5731797e-02, -1.3671625e-01,  3.8269445e-02,\n",
       "          5.0476199e-05,  2.7038497e-03,  2.2076104e-02, -2.4489904e-02],\n",
       "        dtype=float32),\n",
       "  ' Unmarried': array([ 0.06169575, -0.0421164 ,  0.00055061, -0.01907569,  0.0323435 ,\n",
       "         -0.08397535,  0.04328536, -0.0031466 ], dtype=float32),\n",
       "  ' Wife': array([ 0.00572242,  0.0283649 , -0.02111505, -0.06730504,  0.03609628,\n",
       "         -0.02907352, -0.10952311,  0.0341847 ], dtype=float32)},\n",
       " 'race': {' Amer-Indian-Eskimo': array([-0.00582858,  0.0836062 , -0.04417005,  0.00567956, -0.07778133,\n",
       "          0.01000229,  0.00212078,  0.00976511], dtype=float32),\n",
       "  ' Asian-Pac-Islander': array([ 0.02656795,  0.02818123,  0.03472963,  0.00132835,  0.07166744,\n",
       "          0.03127908, -0.08723291,  0.03625572], dtype=float32),\n",
       "  ' Black': array([-0.05042513,  0.04566902,  0.01840615,  0.04027951, -0.06091087,\n",
       "          0.02749507,  0.00681698, -0.01831547], dtype=float32),\n",
       "  ' Other': array([-0.06348605,  0.00816222, -0.06746563,  0.01004195,  0.02127047,\n",
       "          0.02142697, -0.0153108 ,  0.0344866 ], dtype=float32),\n",
       "  ' White': array([-0.02708553,  0.04827349, -0.07259204,  0.01628149, -0.04458854,\n",
       "          0.05067852, -0.00954498,  0.07487433], dtype=float32)},\n",
       " 'gender': {' Female': array([-0.01054364, -0.00336436,  0.06479312,  0.02668346,  0.05199938,\n",
       "          0.02444127,  0.02431335, -0.02897191], dtype=float32),\n",
       "  ' Male': array([ 0.06332516, -0.02079946, -0.0130326 ,  0.05533751, -0.04777779,\n",
       "          0.03805618, -0.0682436 ,  0.01615569], dtype=float32)},\n",
       " 'native_country': {' ?': array([-0.02914551,  0.02897752,  0.03981546,  0.00162218,  0.07930668,\n",
       "          0.02509098, -0.07295793, -0.07779579], dtype=float32),\n",
       "  ' Cambodia': array([-0.05347963,  0.0352383 ,  0.05275067,  0.02214531, -0.01774389,\n",
       "          0.06154348, -0.06987158,  0.04116647], dtype=float32),\n",
       "  ' Canada': array([ 0.04745448, -0.00517465, -0.0824083 , -0.03211503,  0.0547297 ,\n",
       "         -0.00380691, -0.0183793 , -0.07795604], dtype=float32),\n",
       "  ' China': array([-0.0434077 ,  0.03598856,  0.01007734, -0.04167465, -0.06681277,\n",
       "          0.1056304 ,  0.02191541,  0.01157234], dtype=float32),\n",
       "  ' Columbia': array([ 0.05875296, -0.06133858, -0.05514105,  0.05943149, -0.01189835,\n",
       "          0.02795357, -0.03272529, -0.04583446], dtype=float32),\n",
       "  ' Cuba': array([-0.00699126, -0.00648082, -0.00842683,  0.03153045,  0.04242601,\n",
       "         -0.00872155,  0.07218228,  0.0332793 ], dtype=float32),\n",
       "  ' Dominican-Republic': array([-0.06375521, -0.0226606 , -0.08712786, -0.04089175,  0.05240262,\n",
       "          0.03876907,  0.0050834 ,  0.10579135], dtype=float32),\n",
       "  ' Ecuador': array([-0.08713444,  0.00408342,  0.05892631, -0.03347017, -0.01303395,\n",
       "         -0.0667325 ,  0.05081438, -0.01189672], dtype=float32),\n",
       "  ' El-Salvador': array([-0.00438242, -0.08762015, -0.03292462, -0.01350681,  0.03886899,\n",
       "         -0.00096316,  0.02205274,  0.06290862], dtype=float32),\n",
       "  ' England': array([ 0.04954223, -0.02965722, -0.01195973, -0.05484882,  0.00892501,\n",
       "         -0.05103377,  0.0838149 , -0.04426406], dtype=float32),\n",
       "  ' France': array([ 0.02290684, -0.07361806, -0.07745503, -0.0101598 ,  0.01469571,\n",
       "         -0.01091922,  0.00582668, -0.02571213], dtype=float32),\n",
       "  ' Germany': array([ 0.0535339 , -0.02425865, -0.0814742 ,  0.0193314 , -0.01263121,\n",
       "          0.04227909,  0.03861067, -0.06396642], dtype=float32),\n",
       "  ' Greece': array([ 0.01539704, -0.03342623,  0.00959985,  0.06423108,  0.03691233,\n",
       "          0.02581969, -0.00885049, -0.08689382], dtype=float32),\n",
       "  ' Guatemala': array([ 0.00601237, -0.03822167, -0.00538382,  0.00421465,  0.06216006,\n",
       "         -0.0194396 , -0.01290425, -0.04570604], dtype=float32),\n",
       "  ' Haiti': array([ 0.03266804,  0.10874113, -0.05018543, -0.08662044, -0.01256009,\n",
       "         -0.05945445,  0.02774856, -0.01564789], dtype=float32),\n",
       "  ' Holand-Netherlands': array([-0.05812938, -0.03723229, -0.03580297, -0.0646867 , -0.07383658,\n",
       "          0.06269564, -0.05336872, -0.00376311], dtype=float32),\n",
       "  ' Honduras': array([ 0.03870407,  0.01713623, -0.05251431, -0.05378794,  0.12893751,\n",
       "         -0.05316903,  0.02916649,  0.03999162], dtype=float32),\n",
       "  ' Hong': array([-0.05011687,  0.02261239,  0.01635488,  0.01164558, -0.03606608,\n",
       "          0.05359589, -0.02480125,  0.11571202], dtype=float32),\n",
       "  ' Hungary': array([ 0.00444124, -0.00041496, -0.07709275, -0.0182632 , -0.00444144,\n",
       "         -0.05981954,  0.08247036, -0.02498987], dtype=float32),\n",
       "  ' India': array([-0.02054649,  0.06499309,  0.02003016,  0.02843207, -0.04222459,\n",
       "          0.09834574, -0.01857879,  0.05548368], dtype=float32),\n",
       "  ' Iran': array([ 0.01905132, -0.03282222, -0.04967146,  0.01442932, -0.05054822,\n",
       "          0.02894629,  0.00020822, -0.07853222], dtype=float32),\n",
       "  ' Ireland': array([-0.01997488, -0.06905498,  0.04114114, -0.01448977, -0.0330179 ,\n",
       "          0.02212636,  0.01637022, -0.07022718], dtype=float32),\n",
       "  ' Italy': array([-0.00081826, -0.03615477, -0.03354824,  0.0463261 ,  0.01837166,\n",
       "          0.02946142, -0.01512404, -0.04656504], dtype=float32),\n",
       "  ' Jamaica': array([-0.02514315,  0.07520675, -0.02214431, -0.05472546, -0.00051434,\n",
       "         -0.03716812,  0.07794665, -0.0333662 ], dtype=float32),\n",
       "  ' Japan': array([-0.03735174,  0.04750112,  0.01638532,  0.03944055,  0.0513919 ,\n",
       "          0.04607448, -0.01947504, -0.0334322 ], dtype=float32),\n",
       "  ' Laos': array([-0.05500694,  0.03757554,  0.00107482,  0.01912366, -0.0764825 ,\n",
       "          0.1096987 , -0.01913645, -0.01867742], dtype=float32),\n",
       "  ' Mexico': array([-0.03725758, -0.05386824,  0.0363865 , -0.02341149,  0.03826918,\n",
       "         -0.01789302,  0.01783767,  0.04521178], dtype=float32),\n",
       "  ' Nicaragua': array([-0.06457905,  0.00030036,  0.05694565,  0.06805828,  0.02351975,\n",
       "          0.03475036,  0.06733083, -0.00256612], dtype=float32),\n",
       "  ' Outlying-US(Guam-USVI-etc)': array([-0.02481859, -0.00295866,  0.04755395,  0.05836302, -0.01995076,\n",
       "         -0.01773814,  0.01805685,  0.03389935], dtype=float32),\n",
       "  ' Peru': array([ 0.07087769, -0.03759961, -0.02011277, -0.06405307,  0.06412064,\n",
       "          0.01706177,  0.03361427, -0.00556384], dtype=float32),\n",
       "  ' Philippines': array([-0.05023188, -0.00179127, -0.02607799, -0.06686238, -0.04977148,\n",
       "          0.09144192,  0.01023903,  0.01620408], dtype=float32),\n",
       "  ' Poland': array([ 0.04345806, -0.05685567, -0.07125137, -0.01657433, -0.0055305 ,\n",
       "          0.04294243,  0.04201093, -0.04563925], dtype=float32),\n",
       "  ' Portugal': array([ 0.02514999, -0.04495466, -0.01008738, -0.02466705,  0.06992129,\n",
       "         -0.03813183,  0.0065436 ,  0.03847871], dtype=float32),\n",
       "  ' Puerto-Rico': array([-0.06304491,  0.0216438 ,  0.03013482,  0.02667056,  0.09572744,\n",
       "          0.0112728 , -0.07255643, -0.02874872], dtype=float32),\n",
       "  ' Scotland': array([ 0.08306948,  0.01840678, -0.04389641, -0.00412519,  0.00044651,\n",
       "          0.02091576, -0.03018776, -0.08700779], dtype=float32),\n",
       "  ' South': array([-0.04100813,  0.09024289,  0.04433403, -0.02605802, -0.02829475,\n",
       "          0.08681355, -0.03773081, -0.00868799], dtype=float32),\n",
       "  ' Taiwan': array([-0.0149485 ,  0.02375485, -0.03777052,  0.02590469, -0.03607498,\n",
       "          0.06985259, -0.04793686,  0.02333543], dtype=float32),\n",
       "  ' Thailand': array([-0.01989863, -0.00907763, -0.04391263, -0.02745178,  0.01405557,\n",
       "          0.04511881, -0.09161355,  0.04166986], dtype=float32),\n",
       "  ' Trinadad&Tobago': array([-0.07637479,  0.03336807,  0.02127895, -0.04140099, -0.02887266,\n",
       "          0.03323384,  0.09756956, -0.05328169], dtype=float32),\n",
       "  ' United-States': array([ 0.03048798, -0.00502351,  0.01555761,  0.04652237,  0.01429611,\n",
       "         -0.02225815,  0.02220036, -0.1233684 ], dtype=float32),\n",
       "  ' Vietnam': array([-0.04609237,  0.06867181,  0.02844102, -0.0135261 , -0.03459032,\n",
       "          0.08439936, -0.02583349,  0.08428904], dtype=float32),\n",
       "  ' Yugoslavia': array([-0.00714845, -0.04408783, -0.01375567,  0.04556755,  0.05508939,\n",
       "         -0.00768114,  0.00543366,  0.01374232], dtype=float32)}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = {}\n",
    "\n",
    "for layer in model.layers: \n",
    "    if \"_embedding\" in  layer.get_config()[\"name\"]:\n",
    "        col_name = layer.get_config()[\"name\"].split(\"_embedding\")[0]\n",
    "        if col_name not in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "            continue\n",
    "        embeddings[col_name] = {}\n",
    "        for idx, cat in enumerate(CATEGORICAL_FEATURES_WITH_VOCABULARY[col_name]):\n",
    "            if \"mask\" in cat:\n",
    "                continue\n",
    "            embeddings[col_name][cat] = layer.get_weights()[0][idx]\n",
    "            \n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d29c6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unsupervised_trained_embeddings.dictionary', 'wb') as config_dictionary_file:\n",
    "    pickle.dump(embeddings, config_dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfda8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4662b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
